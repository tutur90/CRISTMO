

run_name: lstm_15

output_dir: checkpoints/inv
overwrite_output_dir: true

# === Données ===
data_path: ../data/dataset/
symbols: "BTCUSDT,ETHUSDT,BNBUSDT,XRPUSDT,SUIUSDT,SOLUSDT,ADAUSDT,DOGEUSDT,LTCUSDT,DOTUSDT"
tgt_symbol: "BNBUSDT"
start_date: "2020-01-01"

# pretrained_model: checkpoints/lstm_mse/model.safetensors  ?

# === Modèle ===

src_length: 96
tgt_length: 4
seg_length: 15
type: lstm
input_dim: 4
hidden_dim: 128
output_dim: 8192
num_layers: 2
dropout: 0.0  # Add dropout for regularization
bidirectional: false  # Set to true for bidirectional LSTM if supported
conv_kernel: 5
pool_kernel: 1
loss_function: 
   type: inv
   leverage: 1.0
   grid_scale: 1.0
   softmax: false
   fee: 0.01


# === Entraînement ===
do_eval: true
do_train: true
per_device_train_batch_size: 256
per_device_eval_batch_size: 256
# total_batch_size: 64
# gradient_accumulation_steps: 16

max_steps: 10000
save_steps: 1000
eval_delay: 500
eval_steps: 500
eval_strategy: steps
prediction_loss_only: false
load_best_model_at_end: true
metric_for_best_model: loss
greater_is_better: false
disable_tqdm: false

ddp_find_unused_parameters: false
logging_steps: 100
save_total_limit: 3
report_to: ["wandb"]


dataloader_num_workers: 32
dataloader_drop_last: false
dataloader_pin_memory : true


learning_rate: 4.e-4
lr_scheduler_type: cosine
warmup_steps: 500
# max_grad_norm: 2.0
# weight_decay: 0.0005

include_for_metrics: ["loss"]


bf16: false
fp16: true

accelerator_config:
  split_batches: false
  dispatch_batches: false
  even_batches: true
  use_seedable_sampler: true
  non_blocking: false
