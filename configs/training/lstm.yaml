

run_name: lstm 

output_dir: checkpoints/lstm

# === Données ===
data_path: data/futures/dataset/

# === Modèle ===

src_length: 24
tgt_length: 1
seg_length: 60
type: lstm
input_dim: 4
hidden_dim: 128
output_dim: 3
num_layers: 2
dropout: 0.1  # Add dropout for regularization
bidirectional: false  # Set to true for bidirectional LSTM if supported



# === Entraînement ===
do_train: true
per_device_train_batch_size: 64
per_device_eval_batch_size: 64
# total_batch_size: 64 
# gradient_accumulation_steps: 16

max_steps: 10000
save_steps: 1000
eval_delay: 2000
eval_steps: 1000
eval_strategy: steps
prediction_loss_only: true
load_best_model_at_end: true
metric_for_best_model: loss
greater_is_better: false
disable_tqdm: false

ddp_find_unused_parameters: false
logging_steps: 100
save_total_limit: 5
report_to: []


dataloader_num_workers: 16
dataloader_drop_last: false
dataloader_pin_memory : true


learning_rate: 8.e-4
lr_scheduler_type: inverse_sqrt
warmup_steps: 500
max_grad_norm: 1.0
weight_decay: 0.0005


bf16: false
fp16: true

accelerator_config:
  split_batches: false
  dispatch_batches: false
  even_batches: true
  use_seedable_sampler: true
  non_blocking: false
