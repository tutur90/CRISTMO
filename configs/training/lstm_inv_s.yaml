

run_name: inv 

output_dir: checkpoints/lstm_inv
overwrite_output_dir: true

# === Données ===
data_path: data/futures/dataset/
symbols: "BTCUSDT,ETHUSDT,BNBUSDT,XRPUSDT,SUIUSDT,SOLUSDT,ADAUSDT,DOGEUSDT,LTCUSDT,DOTUSDT"
tgt_symbol: "BTCUSDT"
start_date: "2020-01-01"

# pretrained_model: checkpoints/lstm/checkpoint-8000/model.safetensors

# === Modèle ===

src_length: 
tgt_length: 1
seg_length: 60
type: lstm
input_dim: 4
hidden_dim: 64
output_dim: 3
num_layers: 2
dropout: 0.0  # Add dropout for regularization
bidirectional: false  # Set to true for bidirectional LSTM if supported
conv_kernel: 5
pool_kernel: 1
loss_function: basic_inv

# === Entraînement ===
do_eval: true
do_train: true
per_device_train_batch_size: 128
per_device_eval_batch_size: 128
# total_batch_size: 64
# gradient_accumulation_steps: 16

max_steps: 10000
save_steps: 1000
eval_delay: 500
eval_steps: 500
eval_strategy: steps
prediction_loss_only: true
load_best_model_at_end: true
metric_for_best_model: loss
greater_is_better: false
disable_tqdm: false

ddp_find_unused_parameters: false
logging_steps: 100
save_total_limit: 3
report_to: ["wandb"]


dataloader_num_workers: 16
dataloader_drop_last: false
dataloader_pin_memory : true


learning_rate: 1.e-4
lr_scheduler_type: cosine
warmup_steps: 500
# max_grad_norm: 2.0
# weight_decay: 0.0005


bf16: false
fp16: true

accelerator_config:
  split_batches: false
  dispatch_batches: false
  even_batches: true
  use_seedable_sampler: true
  non_blocking: false
